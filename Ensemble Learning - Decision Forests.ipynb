{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Forests\n",
    "\n",
    "Decision forests are ensemble learning methods that train multiple decision trees. Using multiple decision trees is a way to combat the problem of overfitting that happens commonly in single decision trees.\n",
    "\n",
    "## Bootstrap Aggregating (Bagging)\n",
    "\n",
    "Bagging is an ensemble learning method that uses both bootstrapping and aggregating. Though bagging is classifier-agnostic, it is very common to use decision trees as the classifiers.\n",
    "\n",
    "Bootstrapping is the random sampling of a data set with replacement. Data points from the original data set that are not included in the bootstrapped sample are called out-of-bag samples. Multiple instances of classifiers are trained on multiple bootstrapped data sets. One bootstrapped data set gets a classifier trained on it. The error the classifiers make when predicting on out-of-bag data is called the out-of-bag error.\n",
    "\n",
    "The aggregating part comes in when these classifiers are to vote on the actual prediction. Each classifier gets an equal vote. The most frequent prediction is the final prediction the bagging classifier returns. If using regression trees instead of classification trees, the mean prediction is returned instead.\n",
    "\n",
    "### Basic Algorithm\n",
    "- For $t=1,\\dots,T$:\n",
    "    - Resample the data set with replacement until you have as many samples as in the original.\n",
    "    - Train a classifier (typically a decision tree) on the resampled data\n",
    "- Output final prediction:\n",
    "    - Most frequently predicted class if a classification problem\n",
    "    - Mean of predictions if a regression problem\n",
    "\n",
    "### Example\n",
    "Sample from the iris dataset:\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr><th>Sepal Length</th><th>Sepal Width</th><th>Petal Length</th><th>Petal Width</th><th>Species</th></tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Setosa</td></tr>\n",
    "        <tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Setosa</td></tr>\n",
    "        <tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Setosa</td></tr>\n",
    "        <tr><td>7.0</td><td>3.2</td><td>4.7</td><td>1.4</td><td>Versicolor</td></tr>\n",
    "        <tr><td>6.4</td><td>3.2</td><td>4.5</td><td>1.5</td><td>Versicolor</td></tr>\n",
    "        <tr><td>6.9</td><td>3.1</td><td>4.9</td><td>1.5</td><td>Versicolor</td></tr>\n",
    "        <tr><td>6.5</td><td>3.0</td><td>5.2</td><td>2.0</td><td>Virginica</td></tr>\n",
    "        <tr><td>6.2</td><td>3.4</td><td>5.4</td><td>2.3</td><td>Virginica</td></tr>\n",
    "        <tr><td>6.9</td><td>3.0</td><td>5.1</td><td>1.8</td><td>Virginica</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Bootstrapped data set:\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr><th>Sepal Length</th><th>Sepal Width</th><th>Petal Length</th><th>Petal Width</th><th>Species</th></tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Setosa</td></tr>\n",
    "        <tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Setosa</td></tr>\n",
    "        <tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Setosa</td></tr>\n",
    "        <tr><td>6.5</td><td>3.0</td><td>5.2</td><td>2.0</td><td>Virginica</td></tr>\n",
    "        <tr><td>6.4</td><td>3.2</td><td>4.5</td><td>1.5</td><td>Versicolor</td></tr>\n",
    "        <tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Setosa</td></tr>\n",
    "        <tr><td>6.5</td><td>3.0</td><td>5.2</td><td>2.0</td><td>Virginica</td></tr>\n",
    "        <tr><td>6.2</td><td>3.4</td><td>5.4</td><td>2.3</td><td>Virginica</td></tr>\n",
    "        <tr><td>6.4</td><td>3.2</td><td>4.5</td><td>1.5</td><td>Versicolor</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Out-of-bag data:\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr><th>Sepal Length</th><th>Sepal Width</th><th>Petal Length</th><th>Petal Width</th><th>Species</th></tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr><td>7.0</td><td>3.2</td><td>4.7</td><td>1.4</td><td>Versicolor</td></tr>\n",
    "        <tr><td>6.9</td><td>3.1</td><td>4.9</td><td>1.5</td><td>Versicolor</td></tr>\n",
    "        <tr><td>6.9</td><td>3.0</td><td>5.1</td><td>1.8</td><td>Virginica</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "The bagging classifier helps fix the overfitting problem often seen in decision trees because while single decision trees will be sensitive to noise in the data set, averaging over multiple trees (in practice, hundreds or thousands of them) will solve this problem as long as the trees are not highly correlated to each other. Training multiple trees on the exact same data set will give highly correlated trees. The purpose of bootstraping then is to de-correlate these trees.\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "The idea behind random forests is almost identical to bagging except for one minor difference: while the decision trees in the bagging classifiers are free to split on any of the features at any point in their growth, the decision trees in random forests only choose the best split in $k$ randomly selected features at every split. The reason this is done is to increase randomness in feature selection when splitting because if there is a single variable that splits the data set well, the decision trees in the bagging classifier will always choose this feature, causing these trees to be correlated in that way.\n",
    "\n",
    "A heuristic for choosing $k$ in a classification problem is $k=\\sqrt{n}$ (rounded down), where $n$ is the total number of features. For regression problems, $k=\\frac{n}{3}$ (rounded down) with a minimum node size of $5$ has been suggested by the inventors of the method.\n",
    "\n",
    "### Basic Algorithm\n",
    "- For $t=1,\\dots,T$:\n",
    "    - Resample the data set with replacement\n",
    "    - Train a modified decision tree on the resampled data\n",
    "        - While decision tree's stopping condition for splitting is not met:\n",
    "            - Randomly choose $k$ features\n",
    "            - For each $k$:\n",
    "                - Evaluate all possible split points\n",
    "            - Use best split point\n",
    "- Output final prediction:\n",
    "    - Most frequently predicted class if a classification problem\n",
    "    - Mean of predictions if a regression problem\n",
    "\n",
    "### Example\n",
    "\n",
    "To use the example above, a bagging classifier will train one of its decision trees on this bootstrapped data set:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr><th>Sepal Length</th><th>Sepal Width</th><th>Petal Length</th><th>Petal Width</th><th>Species</th></tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Setosa</td></tr>\n",
    "        <tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Setosa</td></tr>\n",
    "        <tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Setosa</td></tr>\n",
    "        <tr><td>6.5</td><td>3.0</td><td>5.2</td><td>2.0</td><td>Virginica</td></tr>\n",
    "        <tr><td>6.4</td><td>3.2</td><td>4.5</td><td>1.5</td><td>Versicolor</td></tr>\n",
    "        <tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Setosa</td></tr>\n",
    "        <tr><td>6.5</td><td>3.0</td><td>5.2</td><td>2.0</td><td>Virginica</td></tr>\n",
    "        <tr><td>6.2</td><td>3.4</td><td>5.4</td><td>2.3</td><td>Virginica</td></tr>\n",
    "        <tr><td>6.4</td><td>3.2</td><td>4.5</td><td>1.5</td><td>Versicolor</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Training the decision tree proceeds as it normally does.\n",
    "\n",
    "But in a random forest, a decision tree will randomly choose $k$ features on which to find the optimal split. So for instance, it might start only looking for the optimal split point in Sepal Length and Petal Length (when $k=2$). When the child nodes split, the decision tree will again randomly choose $2$ features in which to find the optimal split point. \n",
    "\n",
    "## Extremely Randomized Trees (ExtraTrees)\n",
    "\n",
    "The ExtraTrees algorithm presents another way of randomizing trees. It does not use bootstrapped data and trains several trees on the whole data set, but it randomizes the splits. As in a random forest, $k$ features are randomly chosen. But instead of evaluating all the split points in these $k$ features and getting the optimal one, a random split point is chosen for each feature using a uniform distribution on the range of possible values for said feature. There are now $k$ split points and the best one among these is chosen. Because ExtraTrees does not evaluate all the possible split points, training is a lot faster.\n",
    "\n",
    "### Basic Algorithm\n",
    "- For $t=1,\\dots,T$:\n",
    "    - Train a modified decision tree on the original data\n",
    "        - While decision tree's stopping condition for splitting is not met:\n",
    "            - Randomly choose $k$ features\n",
    "            - For each $k$:\n",
    "                - Randomly choose one split point using a uniform distribution on the range of possible values for feature $k$\n",
    "            - Evaluate all $k$ split points\n",
    "            - Choose best split point\n",
    "- Output final prediction:\n",
    "    - Most frequently predicted class if a classification problem\n",
    "    - Mean of predictions if a regression problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
