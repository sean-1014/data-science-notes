{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "The decision tree represents observations about an item in branches (splits) and represents a target value in leaf nodes. The splits could be thought of as questions. For example, \"is a person male or female?\" is a possible way to split the data points. If the target variable is whether or not they survived the sinking of the Titanic, then the leaf nodes represent \"yes\" or \"no\", which is the tree's final decision (prediction). If the target variable is categorical, the decision tree is a **classification tree**. If the target variable is numeric, the decision tree is a **regression tree**.\n",
    "\n",
    "## Classification Trees\n",
    "\n",
    "### Branching Out\n",
    "\n",
    "Decision trees usually start out at a root node. The first split is performed by choosing a question to ask. This is the question that best separates the data by some metric. Different metrics can be used but common ones are Gini impurity and information gain. It has been shown in practice that Gini impurity and information gain perform almost equally well, so given that Gini impurity does not require dealing with logarithms, it is a good default metric.\n",
    "\n",
    "Some decision tree algorithms like CHAID allow for multiple splits. But most decision tree algorithms only allow binary splits. This is because of how quickly the number of possible combinations grows.\n",
    "### Gini Impurity\n",
    "\n",
    "Gini impurity measures how often a new data point would be incorrectly labeled if it were to be randomly labeled according to the distribution of the labels in a subset.\n",
    "\n",
    "$$I_G(p) = 1 - \\displaystyle{\\sum_{i=1}^J}p_i^2$$\n",
    "\n",
    "where:\n",
    "- $p_i$ is the probability of belonging to class $i$\n",
    "- $J$ is the number of classes\n",
    "\n",
    "If there is only one class in a subset, then $I_G(p) = 1 - 1^2 = 0$. This means a Gini impurity of $0$ is obtained when there is no mixture of classes in a subset (which would be ideal). When having to choose between possible splits, the split that gives the lowest Gini impurity should be chosen. In this way, the Gini impurity could also be thought of as a measure of the *pureness of the sets of data points produced at a node*.\n",
    "\n",
    "A split partitions the data set into smaller sets, each of which has its own Gini impurity. The total Gini impurity for the split is calculated by a weighted average of the Gini impurity values. The weights are determined simply by the proportion of the number of data points in each subset.\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "Information gain is another way to measure optimal splits. It is based on the concept of entropy.\n",
    "\n",
    "$$Entropy = -\\displaystyle{\\sum_{i=1}^J}p_i\\log_2p_i$$<br>\n",
    "$$\\text{Information Gain = Entropy of Parent Node}-\\text{Weighted Entropy of Child Nodes}$$\n",
    "\n",
    "where:\n",
    "- $p_i$ is the probability of belonging to class $i$\n",
    "- $J$ is the number of classes\n",
    "\n",
    "Entropy is $0$ when a set is pure because 0 bits are needed to encode this information (i.e. we are fully certain that the elements of this set belong to the same class). Entropy gets higher and higher the more labels are mixed in the set because there is higher uncertainty. But since information gain tells us how much entropy is lost by performing the split, we want to maximize information gain.\n",
    "\n",
    "Information gain tends to be biased towards features that can take on multiple values (e.g. age vs. gender). To correct this, the information gain ratio can be used instead:\n",
    "\n",
    "$$\\text{Information Gain Ratio} = \\displaystyle{\\frac{\\text{Information Gain}}{\\text{Split Information}}}$$<br>\n",
    "$$\\text{Split Information} = -\\displaystyle{\\sum_{i=1}^A}\\frac{|D_i|}{|D|}\\log_2\\frac{|D_i|}{|D|}$$\n",
    "\n",
    "where:\n",
    "- $|D|$ is the total number of data points being considered in the split\n",
    "- $|D_i|$ is the number of data points that have the attribute $i$\n",
    "- $A$ is the number of possible values of the attribute on which the split is to be performed (**do not confuse this for the number of classes**)\n",
    "\n",
    "Split information can be thought of as the total information in the data before the split. This is the same as the entropy of the parent node.\n",
    "\n",
    "### Example\n",
    "\n",
    "Sample from the Titanic data set:\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr><th>Gender</th><th>Passenger Class</th><th>Survived</th></tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr><td>M</td><td>3</td><td>0</td></tr>\n",
    "        <tr><td>F</td><td>1</td><td>1</td></tr>\n",
    "        <tr><td>F</td><td>3</td><td>1</td></tr>\n",
    "        <tr><td>F</td><td>1</td><td>0</td></tr>\n",
    "        <tr><td>M</td><td>3</td><td>0</td></tr>\n",
    "        <tr><td>M</td><td>3</td><td>0</td></tr>\n",
    "        <tr><td>M</td><td>1</td><td>0</td></tr>\n",
    "        <tr><td>M</td><td>3</td><td>1</td></tr>\n",
    "        <tr><td>F</td><td>3</td><td>1</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "#### Gini Impurity\n",
    "\n",
    "#### Possible Split 1: Gender\n",
    "\n",
    "**Male**\n",
    "\n",
    "$Pr(Survived=1|Gender=Male) = \\frac{1}{5}$<br>\n",
    "$Pr(Survived=0|Gender=Male) = \\frac{4}{5}$<br>\n",
    "$I_GMale = 1 - \\displaystyle{\\sum_{i=1}^J} p_i^2 = 1 - \\left(\\frac{1}{5}^2 + \\frac{4}{5}^2\\right) = 0.32$<br>\n",
    "\n",
    "**Female**\n",
    "\n",
    "$Pr(Survived=1|Gender=Female) = \\frac{3}{4}$<br>\n",
    "$Pr(Survived=0|Gender=Female) = \\frac{1}{4}$<br>\n",
    "$I_GFemale = 1 - \\displaystyle{\\sum_{i=1}^J} p_i^2 = 1 - \\left(\\frac{3}{4}^2 + \\frac{1}{4}^2\\right) = 0.375$<br>\n",
    "\n",
    "$$Weighted\\ I_GGender = \\displaystyle{\\frac{5}{9}}(0.32) + \\frac{4}{9}(0.375) = 0.344$$\n",
    "\n",
    "#### Possible Split 2: Passenger Class\n",
    "\n",
    "**Passenger Class 3**\n",
    "\n",
    "$Pr(Survived=1|Passenger\\ Class=3) = \\frac{3}{6}$<br>\n",
    "$Pr(Survived=0|Passenger\\ Class=3) = \\frac{3}{6}$<br>\n",
    "$I_GMale = 1 - \\displaystyle{\\sum_{i=1}^J} p_i^2 = 1 - \\left(\\frac{3}{6}^2 + \\frac{3}{6}^2\\right) = 0.5$<br>\n",
    "\n",
    "**Passenger Class 1**\n",
    "\n",
    "$Pr(Survived=1|Passenger\\ Class=1) = \\frac{1}{3}$<br>\n",
    "$Pr(Survived=0|Passenger\\ Class=1) = \\frac{2}{3}$<br>\n",
    "$I_GFemale = 1 - \\displaystyle{\\sum_{i=1}^J} p_i^2 = 1 - \\left(\\frac{1}{3}^2 + \\frac{2}{3}^2\\right) = 0.44$<br>\n",
    "\n",
    "$$Weighted\\ I_GPassenger\\ Class = \\displaystyle{\\frac{6}{9}}(0.5) + \\frac{3}{9}(0.375) = 0.48$$\n",
    "\n",
    "Gender is the better predictor, so if we have to choose between these two, we'd want to split on gender.\n",
    "\n",
    "#### Information Gain\n",
    "\n",
    "#### Possible Split 1: Gender\n",
    "\n",
    "**Male**\n",
    "\n",
    "$Pr(Survived=1|Gender=Male) = \\frac{1}{5}$<br>\n",
    "$Pr(Survived=0|Gender=Male) = \\frac{4}{5}$<br>\n",
    "$\\text{Entropy}_{Male} = \\displaystyle{-\\frac{1}{5}}\\log_2\\frac{1}{5}-\\frac{4}{5}\\log_2\\frac{4}{5} \\approx 0.722$<br>\n",
    "\n",
    "**Female**\n",
    "\n",
    "$Pr(Survived=1|Gender=Female) = \\frac{3}{4}$<br>\n",
    "$Pr(Survived=0|Gender=Female) = \\frac{1}{4}$<br>\n",
    "$\\text{Entropy}_{Female} = \\displaystyle{-\\frac{3}{4}}\\log_2\\frac{3}{4}-\\frac{1}{4}\\log_2\\frac{1}{4} \\approx 0.811$<br><br>\n",
    "\n",
    "$\\text{Entropy}_{Parent}\\text{(5 dead : 4 alive)} = -\\frac{5}{9}\\log_2\\frac{5}{9} - \\frac{4}{9}\\log_2\\frac{4}{9} \\approx 0.991$<br>\n",
    "$\\text{InfoGain}_{Gender} = 0.991 - \\frac{5}{9}(0.722) - \\frac{4}{9}(0.811) \\approx 0.229$<br>\n",
    "$\\text{Split Information (5 male : 4 female)}=-\\frac{5}{9}\\log_2\\frac{5}{9}-\\frac{4}{9}\\log_2\\frac{4}{9} \\approx 0.991$<br><br>\n",
    "$$\\text{InfoGainRatio}_{Gender} \\approx \\displaystyle{\\frac{0.229}{0.991}} \\approx 0.232$$\n",
    "\n",
    "#### Possible Split 2: Passenger Class\n",
    "\n",
    "**Passenger Class 3**\n",
    "\n",
    "$Pr(Survived=1|Passenger\\ Class=3) = \\frac{3}{6}$<br>\n",
    "$Pr(Survived=0|Passenger\\ Class=3) = \\frac{3}{6}$<br>\n",
    "$\\text{Entropy}_{PassClass1} = \\displaystyle{-\\frac{3}{6}}\\log_2\\frac{3}{6}-\\frac{3}{6}\\log_2\\frac{3}{6} = 1$<br>\n",
    "\n",
    "**Passenger Class 1**\n",
    "\n",
    "$Pr(Survived=1|Passenger\\ Class=1) = \\frac{1}{3}$<br>\n",
    "$Pr(Survived=0|Passenger\\ Class=1) = \\frac{2}{3}$<br>\n",
    "$\\text{Entropy}_{PassClass3} = \\displaystyle{-\\frac{1}{3}}\\log_2\\frac{1}{3}-\\frac{2}{3}\\log_2\\frac{2}{3} \\approx 0.918$<br><br>\n",
    "\n",
    "$\\text{Entropy}_{Parent} \\approx 0.991$<br>\n",
    "$\\text{InfoGain}_{PassClass} = 0.991 - \\frac{6}{9}(1) - \\frac{3}{9}(0.918) \\approx 0.018$<br>\n",
    "$\\text{Split Information (6 PassClass3 : 3 PassClass1)} = -\\frac{6}{9}\\log_2\\frac{6}{9}-\\frac{3}{9}\\log_2\\frac{3}{9} \\approx 0.918$<br><br>\n",
    "\n",
    "$$\\text{InfoGainRatio}_{PassClass} \\approx \\displaystyle{\\frac{0.018}{0.918}} \\approx 0.02$$\n",
    "\n",
    "Gender is again the better predictor by this metric.\n",
    "\n",
    "### More on Branching Out\n",
    "\n",
    "The example provided above is simple because the features to split on are binary. But what if they were continuous variables like age? Different packages handle this differently but they evaluate the different values at which a split could be made. For instance, if the ages are 10, 25, 16, 34, and 31, one way to go about this would be to sort the ages then perform the split on age < 16, then evaluate, then do the same for all other potential binary splits on age. Another way to choose split points would be to calculate the averages between adjacent numeric values (e.g. average of 10 and 16, average of 16 and 25, etc.) and use the averages as split points (e.g. age $\\leq$ 13, age $\\leq$ 20.5, etc.). Then the best split can be chosen.\n",
    "\n",
    "In the example above, passenger class is technically categorical but the sample only featured two passenger classes. If the example included passenger class 2, then there would have been more possible ways to split:\n",
    "- 1 vs. 2 or 3\n",
    "- 2 vs. 1 or 3\n",
    "- 3 vs. 1 or 2\n",
    "\n",
    "If there were four categorical values, these are all the possible binary splits:\n",
    "- 1 vs. 2 or 3 or 4\n",
    "- 2 vs. 1 or 3 or 4\n",
    "- 3 vs. 1 or 2 or 4\n",
    "- 4 vs. 1 or 2 or 3\n",
    "- 1 or 2 vs. 3 or 4\n",
    "- 1 or 3 vs. 2 or 4\n",
    "- 1 or 4 vs. 2 or 3\n",
    "\n",
    "In general, the number of possible binary splits is $2^{k-1}-1$, where $k$ is the number of levels in the categorical variable.\n",
    "\n",
    "One nice property of the decision tree is that at each split, it only ever considers one variable once it has found the optimal split point. This means that the scale of the variables is irrelevant because they don't affect each other, so feature scaling is not necessary when working with decision trees.\n",
    "\n",
    "### Termination\n",
    "\n",
    "After the first split, the same general procedure is used to split the child nodes further. But branching out will only be productive if performing a split lowers the Gini impurity or improves the information gain. If at one node, all other possible splits raise the Gini impurity or reduce the information gain, no more splits will be performed and that node becomes a leaf node.\n",
    "\n",
    "## Regression Trees\n",
    "\n",
    "Regression trees partition the data set into \"regions\" when they perform splits. The $y$ returned in a region is the average of all the data points in the region. This means the regression tree is essentially a piecewise constant function.\n",
    "\n",
    "### Visualization of a Regression Tree\n",
    "\n",
    "Below is a quick visualization of what a very simple regression tree looks like. Given one predictor $X$ and the target values $Y$, the decision tree partitions the data points into three regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1,10,.1)\n",
    "r1_len = int(len(x)/2)\n",
    "r2_len = int(r1_len/2)\n",
    "r3_len = len(x)-r1_len-r2_len\n",
    "y_r1 = 20 + np.random.normal(0,1,r1_len)\n",
    "y_r2 = -10 + np.random.normal(0,1,r2_len)\n",
    "y_r3 = np.random.normal(0,1,r3_len)\n",
    "y = np.concatenate((y_r1,y_r2,y_r3))\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "axs[0].scatter(x,y)\n",
    "axs[0].plot(x[:r1_len],np.full(r1_len,y_r1.mean()),color='red')\n",
    "axs[0].plot(x[r1_len:r1_len+r2_len],np.full(r2_len,y_r2.mean()),color='red')\n",
    "axs[0].plot(x[-r3_len:],np.full(r3_len,y_r3.mean()),color='red')\n",
    "axs[0].set_title('Regression Tree')\n",
    "axs[0].set_xlabel('X')\n",
    "axs[0].set_ylabel('Y')\n",
    "\n",
    "axs[1].set_xlim(1,10)\n",
    "axs[1].set_ylim(0,1)\n",
    "axs[1].axvline(x=x[r1_len],color='black')\n",
    "axs[1].axvline(x=x[r1_len+r2_len],color='black')\n",
    "axs[1].text(3,.5,'Y={:.2f}'.format(y_r1.mean()),horizontalalignment='center',fontsize=12)\n",
    "axs[1].text(6.6,.5,'Y={:.2f}'.format(y_r2.mean()),horizontalalignment='center',fontsize=12)\n",
    "axs[1].text(8.8,.5,'Y={:.2f}'.format(y_r3.mean()),horizontalalignment='center',fontsize=12)\n",
    "axs[1].set_title('Regression Tree Partitions')\n",
    "axs[1].set_xlabel('X')\n",
    "axs[1].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the very simple example above, if any new data point whose $X < 5.5$ comes in, the regression tree's prediction $\\hat{y}$ is equal to the mean of the data points in the left region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Regression Tree Region Partitioning](img/decision_trees/regression_tree_region_partitioning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this slightly more complicated example, there are two predictors $X_1$ and $X_2$ and the input space is divided into five regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Regression Tree Outputs](img/decision_trees/regression_tree_outputs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are five regions, there are five different possible outputs represented by the different levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Tree Splits\n",
    "\n",
    "In a regression tree, one way to find the optimal split point is to get the point that minimizes the sum of the squared errors across two regions $R_1$ and $R_2$:\n",
    "\n",
    "$$\\text{minimize}\\quad\\displaystyle{\\sum_{i \\in R_1}}(y_i-\\hat{y}_{R_1})^2+\\sum_{i \\in R_2}(y_i-\\hat{y}_{R_2})^2$$\n",
    "\n",
    "where:\n",
    "- $\\hat{y}_{R_1}$ and $\\hat{y}_{R_2}$ are the means of the $y$s of all the data points in regions $R_1$ and $R_2$ respectively\n",
    "\n",
    "The search for the best split starts with all the possible split points across all predictors. The splits are performed until some stopping condition is met.\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "Decision trees are known to overfit on the data they are trained on. If left to grow unchecked, the decision tree can grow in such a way that some leaves have single data points because the Gini impurity will always decrease until leaves are pure sets. Tuning some `sklearn` decision tree parameters can help combat this:\n",
    "- `max_depth` - the maximum depth of the decision tree\n",
    "- `min_samples_split` - the minimum number of samples required in a node to consider splitting\n",
    "- `min_samples_leaf` - the minimum number of samples a leaf node can have (a split won't be performed if it results in a leaf node with fewer data points than this)\n",
    "- `max_features` - the maximum number of features to consider when looking for the best split\n",
    "\n",
    "Preventing the growth of the tree by naïvely controlling the leaf and tree sizes is one way to go about it. But there exist more complex tree pruning methods and both require splitting the data into train-test sets or cross validation sets:\n",
    "- **Pre-pruning** methods stop the tree from growing\n",
    "    - At every stage, evaluate cross-validation error and stop splitting if the error is not significantly reduced\n",
    "- **Post-pruning** methdos allow the tree to overfit first then cut off branches later.\\\n",
    "    - For each node, remove the node and its children and check test error. Remove nodes if validation error actually decreases. Keep pruning until it worsens the test error.\n",
    "    - Reduced error pruning\n",
    "        - start at the leaves and replace the parent node with the most popular class (e.g. if parent node splits into 6 yes - 3 no, just evaluate yes all the time at the parent node, making that a leaf node)\n",
    "        - the replacement is saved if the test error turns out to be lower after the change\n",
    "\n",
    "Post-pruning works better in practice because pre-pruning can cut off learning too early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-pruning Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pruning Example](img/decision_trees/pruning_train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the decision tree above trained on some training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pruning Example](img/decision_trees/pruning_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the test set is used, the errors sum up to $\\frac{6}{20}$ for the two leaves at the left. Since the most popular class in the parent of those leaves was N when it was training, we could check if just predicting N for all data points that end up in that node results in a lower error. The error does turn out to be lower at only $\\frac{5}{20}$, so we can remove the subtree from that node and make that the leaf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
